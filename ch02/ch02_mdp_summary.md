# Chapter 02: 마르코프 의사 결정 과정 (MDP)

## 1. MDP란?

마르코프 의사 결정 과정 (Markov Decision Process, MDP)은 어떤 환경에서 의사 결정 에이전트가 행동을 결정하는 과정. 주로 순차적인 의사 결정을 다루며, 현재 상태가 다음 상태에 영향을 미치지만, 과거의 모든 상태가 아닌 현재 상태만으로 다음 상태를 예측할 수 있다는 마르코프 성질을 기반으로 함.

## 2. 마르코프 성질

- **현재 상태가 주어졌을 때, 미래는 과거와 독립적**
- 다음 상태는 오직 현재 상태와 현재 행동에만 의존
- $P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)$

## 3. MDP의 구성 요소

*   **상태 (State, S) 전이**: 에이전트가 처할 수 있는 모든 가능한 상황들의 집합. 예를 들어, 게임에서 캐릭터의 위치나 체력 등이 상태가 될 수 있음.
*   **행동 (Action, A)**: 에이전트가 특정 상태에서 취할 수 있는 모든 가능한 행동들의 집합. 예를 들어, 앞으로 이동, 점프, 공격 등이 행동이 될 수 있음.
*   **전이 확률 (Transition Probability, P)**: 에이전트가 상태 $s$에서 행동 $a$를 취했을 때, 다음 상태 $s'$으로 전이할 확률 $P(s' | s, a)$. 이는 환경의 역학을 나타냄.
*   **보상 함수 (Reward Function, R)**: 에이전트가 상태 $s$에서 행동 $a$를 취하여 다음 상태 $s'$으로 전이했을 때 받는 즉각적인 보상 $R(s, a, s')$. 에이전트의 목표는 장기적인 보상의 합을 최대화하는 것임.
*   **감가율 (Discount Factor, $\gamma$)**: 미래의 보상이 현재의 보상에 비해 얼마나 가치 있는지를 나타내는 0과 1 사이의 값. $\gamma$가 0에 가까울수록 에이전트는 즉각적인 보상을 선호하고, 1에 가까울수록 장기적인 보상을 중요하게 생각함.

## 4. 정책 (Policy)

정책 $\pi(a | s)$는 특정 상태 $s$에서 어떤 행동 $a$를 취할지에 대한 확률 분포를 나타냄.

- **결정론적 정책**: π(s) = a (상태 s에서 항상 행동 a 선택)
- **확률론적 정책**: π(a|s) (상태 s에서 행동 a를 선택할 확률)

## 5. MDP의 목표

MDP의 목표는 **최적 정책 (Optimal Policy, $\pi^*$)**을 찾는 것임. 최적 정책은 에이전트가 시작부터 끝까지 얻을 수 있는 총 기대 보상 (Expected Return)을 최대화하는 정책임.

## 6. 가치 함수 (Value Function)

가치 함수는 특정 상태나 특정 상태-행동 쌍의 "좋음"을 평가하는 데 사용됨.

*   **상태 가치 함수 (State-Value Function, $V^\pi(s)$)**: 특정 정책 $\pi$를 따랐을 때, 상태 $s$에서 시작하여 얻을 수 있는 총 기대 보상을 나타냄.
    
    $V^\pi(s) = E_\pi[\sum_{t=0}^\infty \gamma^t R_{t+1} | S_t = s]$

*   **행동 가치 함수 (Action-Value Function, $Q^\pi(s, a)$)**: 특정 정책 $\pi$를 따랐을 때, 상태 $s$에서 행동 $a$를 취한 후 얻을 수 있는 총 기대 보상을 나타냄.
    
    $Q^\pi(s, a) = E_\pi[\sum_{t=0}^\infty \gamma^t R_{t+1} | S_t = s, A_t = a]$

### 벨만 방정식

벨만 방정식은 가치 함수를 재귀적으로 정의하는 중요한 방정식임.

*   **벨만 기대 방정식 (Bellman Expectation Equation)**:
    
    $V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^\pi(s')]$
    
    $Q^\pi(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a')]$

## 7. 최적 가치 함수와 최적 정책

*   **최적 상태 가치 함수 ($V^*(s)$)**: 모든 정책 중에서 가장 큰 상태 가치 함수임.
    
    $V^*(s) = \max_\pi V^\pi(s)$

*   **최적 행동 가치 함수 ($Q^*(s, a)$)**: 모든 정책 중에서 가장 큰 행동 가치 함수임.
    
    $Q^*(s, a) = \max_\pi Q^\pi(s, a)$

*   **벨만 최적 방정식 (Bellman Optimality Equation)**:
    
    $V^*(s) = \max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^*(s')]$
    
    $Q^*(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \max_{a'} Q^*(s',a')]$

최적 정책은 최적 행동 가치 함수를 통해 쉽게 도출될 수 있음.

$\pi^*(a|s) = 1 \text{ if } a = \arg\max_{a'} Q^*(s, a') \text{ else } 0$

## 8. GridWorld 예시

3×4 격자 세계에서 에이전트가 목표 지점에 도달하는 문제

```
[0] [0] [0] [+1]  ← 목표 지점 (보상 +1)
[0] [벽] [0] [-1]  ← 함정 (보상 -1)
[S] [0] [0] [0]   ← S: 시작 지점
```

**가능한 행동**: 상(UP), 하(DOWN), 좌(LEFT), 우(RIGHT)

## 9. MDP의 활용

MDP는 동적 계획법(Dynamic Programming)을 통해 해결될 수 있으며, 강화 학습의 이론적 기반이 됨. 정책 이터레이션(Policy Iteration)과 가치 이터레이션(Value Iteration)은 MDP를 푸는 대표적인 알고리즘임.

강화 학습에서는 MDP의 전이 확률이나 보상 함수를 알 수 없는 경우에 에이전트가 환경과 상호작용하면서 최적 정책을 찾아나감.